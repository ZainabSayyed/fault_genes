{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStack_WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import nltk.stem \n",
    "from nltk import stem\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Randomly select 1000 from bugslist and report (launch pad and stack over flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"buglist.xlsx\"\n",
    "df_launchpad=pd.read_excel(filename)\n",
    "df=pd.DataFrame(df_launchpad)\n",
    "del(df_launchpad)\n",
    "df.set_index=df.bug\n",
    "df=df.drop(['project','component','version','fault_class','fault_type','fault_symptom','severity','priority','status','mitigation','log','repro','submitter','assignee','created','deployment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "filename = \"ask_os.csv\"\n",
    "df_ask_os = pd.read_csv(filename)\n",
    "df_ask=pd.DataFrame(df_ask_os)\n",
    "del(df_ask_os)\n",
    "df_ask.set_index=df_ask.url\n",
    "df_ask=df_ask.drop(['Serial','answer'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"report.xlsx\"\n",
    "df_stackoverflow = pd.read_excel(filename)\n",
    "\n",
    "df_st=pd.DataFrame(df_stackoverflow)\n",
    "del(df_stackoverflow)\n",
    "df_st.set_index=df_st.id\n",
    "df_st=df_st.drop(['answers','link','Creator','date','votes','fetch_url'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')\n",
    "stop_word_dict=[]\n",
    "for i in stop_words:\n",
    "    stop_word_dict.append(str(i.encode(\"utf-8\")))\n",
    "common_words=['',\"aren't\",'ffffffffff']\n",
    "stop_word_dict=stop_word_dict + common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dict(dataframe,field,stop_word_dict):\n",
    "    corpus=[]\n",
    "    for i in field:\n",
    "        \n",
    "        if type(i)== int or type(i)== float:  \n",
    "            continue\n",
    "        i = re.sub(r'[^\\x00-\\x7f]','',i)\n",
    "        for k in i:\n",
    "            if type(k) == type(u''):\n",
    "                k = k.encode(\"utf-8\",'ignore')\n",
    "        corpus.append(str(i).strip())\n",
    "    dataframe['field_corpus']=pd.Series(corpus)\n",
    "    dataframe['field_corpus']=pd.Series( str(i).lower()  if len(str(i)) >0 else '' for i in dataframe.field_corpus)\n",
    "    corpus=[]\n",
    "    for i in dataframe.field_corpus:\n",
    "        data=(str(i))\n",
    "        data=re.sub(r'[^\\x00-\\x7f]','',data) or re.sub(r'[0-9]','',data)\n",
    "        corpus.append(data) \n",
    "    dataframe.field_corpus_new=pd.Series(corpus)\n",
    "    corpus=[]\n",
    "    new_corpus=[]\n",
    "    row_corpus=[]\n",
    "    for i in dataframe.field_corpus_new:\n",
    "        substr=re.split('[^A-Za-z]',i)\n",
    "        for j in substr:\n",
    "            j=j.strip()\n",
    "            if j not in stop_word_dict and len(j)>1:\n",
    "                corpus.append(j)   \n",
    "        new_corpus.append(corpus)\n",
    "        corpus=[]\n",
    "    dataframe.field_words_bag=pd.Series(new_corpus)\n",
    "    field_words_count={}\n",
    "    for i in new_corpus:\n",
    "        for w in i:\n",
    "            field_words_count[w]=field_words_count.get(w,0)+1 \n",
    "    return field_words_count,dataframe.field_words_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_words_count={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Fault_Details_lp= pd.Series(df.fault_description+df.req)\n",
    "#df=df.drop(['fault_description','req'],axis=1)\n",
    "df=df.assign(Fault_Details_lp=Fault_Details_lp.values)\n",
    "launchpad_words_count={}\n",
    "launchpad_words_count,df['field_words_bag']=create_dict(df,df.Fault_Details_lp,stop_word_dict)\n",
    "#df=df.drop(['Fault_Details_lp'],axis=1)\n",
    "for k,v in launchpad_words_count.items():\n",
    "    my_words_count[k]=my_words_count.get(k,1)+v\n",
    "del(launchpad_words_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(my_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Fault_Details_st= pd.Series(df_st.title+df_st.tags+ df_st.longtext.to_string())\n",
    "#df_st=df_st.drop(['tags','title','longtext'],axis=1)\n",
    "df_st=df_st.assign(Fault_Details_st=Fault_Details_st.values)\n",
    "stackoverflow_words_count={}\n",
    "stackoverflow_words_count,df_st['field_words_bag']=create_dict(df_st,df_st.Fault_Details_st,stop_word_dict)\n",
    "#df_st=df_st.drop(['Fault_Details_st'],axis=1)\n",
    "for k,v in stackoverflow_words_count.items():\n",
    "    my_words_count[k]=my_words_count.get(k,1)+v\n",
    "del(stackoverflow_words_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(my_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fault_Details_ask = pd.Series(df_ask.summary+df_ask.title)\n",
    "#df_ask=df_ask.drop(['summary','title'],axis=1)\n",
    "df_ask=df_ask.assign(Fault_Details_ask=Fault_Details_ask.values)\n",
    "ask_words_count={}\n",
    "ask_words_count,df_ask['field_words_bag']=create_dict(df_ask,df_ask.Fault_Details_ask,stop_word_dict)\n",
    "#df_ask=df_ask.drop(['Fault_Details_ask'],axis=1)\n",
    "for k,v in ask_words_count.items():\n",
    "    my_words_count[k]=my_words_count.get(k,1)+v\n",
    "del(ask_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(my_words_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove non-ascii character to process data using NLTK\n",
    "#NLTK coverts string into meaningful words called token, removes punctuations and special characters.\n",
    "#Analyzing Fault_desc and Req field from data set\n",
    "#write process data to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#launchpad_words_count={}\n",
    "#launchpad_words_count,df['field_words_bag']=create_dict(df,df.Fault_Details_lp,stop_word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mylist=[]\n",
    "for key, value in sorted(my_words_count.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "    mylist.append([key,value])\n",
    "df_wordcount=pd.DataFrame(mylist, columns =['Key','Count'])\n",
    "#del (mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#del (my_words_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df_wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_wordcount.to_csv('dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"dictionary created\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mydict=set(df_wordcount.Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del (df_wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html\" target=\"_blank\" title=\"nlp.stanford.edu\" style=\"display: block; text-align: center;\"><img src=\"http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html\" alt=\"nlp.stanford.edu\" style=\"max-width: 100%;width: 800px;\"  width=\"800\"/></a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using TF-IDF to convert unstructured text to useful features\n",
    "'''All_dict={}\n",
    "for i in range(0,len(df_wordcount)):\n",
    "    All_dict[df_wordcount.Key[i]]=df_wordcount.Count[i]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def computeTF(All_dict, field):\n",
    "    tfDict = {}\n",
    "    fieldCount= len(field)\n",
    "    for key, value in All_dict.iteritems():\n",
    "        tfDict[key] = value/ float(fieldCount)\n",
    "    return tfDict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''tf_Fault_Details_lp = computeTF(All_dict,df.Fault_Details_lp)\n",
    "tf_Fault_Details_st = computeTF(All_dict,df_st.Fault_Details_st)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict= {}\n",
    "    N= len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(),0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.iteritems():\n",
    "            if val > 0:\n",
    "                idfDict[word]+=1\n",
    "    for word,val in idfDict.iteritems():\n",
    "        idfDict[word] = math.log(N/ float(val))\n",
    "    return idfDict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''idfs_lp=computeIDF([tf_Fault_Details_lp])\n",
    "idfs_st=computeIDF([tf_Fault_Details_st])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''idfs_lp'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def computeTFIDF(tfList, idfs):\n",
    "    tfidf={}\n",
    "    for word,val in tfList.iteritems():\n",
    "        tfidf[word]= val *  idfs[word]\n",
    "    return tfidf'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''tfidf_launchpad_list = computeTFIDF(launchpad_words_count, idfs_lp)\n",
    "tfidf_stackoverflow_list = computeTF(stackoverflow_words_count, idfs_st)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''heading=['Launch_pad','Stack_overflow']\n",
    "tfMatrix=pd.DataFrame([tfidf_launchpad_list,tfidf_stackoverflow_list])\n",
    "Temp=pd.Series(heading)\n",
    "tfMatrix= tfMatrix.assign(Temp=Temp.values)\n",
    "tfMatrix.index=Temp\n",
    "tfMatrix.to_csv('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\tfmatrix - Copy.csv')\n",
    "tfMatrix.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "#stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(df.Fault_Details_lp)\n",
    "\n",
    "print(tfidf_matrix.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''terms = tfidf_vectorizer.get_feature_names()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "%time km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from sklearn.externals import joblib\n",
    "joblib.dump(km,  'C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\LaundPad.pkl')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''km = joblib.load('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\LaundPad.pkl')\n",
    "clusters = km.labels_.tolist()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "Cluster=pd.Series(clusters)\n",
    "df=df.assign(Cluster=Cluster.values)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''df.to_csv('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\Classified_LaunchPad.csv')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create DTM for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''from sklearn.feature_extraction.text import CountVectorizer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''vocab=mydict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''simple_vectorizer = CountVectorizer(vocabulary=vocab,min_df=0., max_df=1.0)\n",
    "bow1 = simple_vectorizer.fit_transform(df.Fault_Details_lp)\n",
    "bow2 = simple_vectorizer.fit_transform(df_st.Fault_Details_st)\n",
    "bow3 = simple_vectorizer.fit_transform(df_ask.Fault_Details_ask)\n",
    "bow1_df=pd.DataFrame(bow1.A, columns=simple_vectorizer.get_feature_names())\n",
    "bow2_df =pd.DataFrame(bow2.A,columns=simple_vectorizer.get_feature_names())\n",
    "bow3_df =pd.DataFrame(bow3.A,columns=simple_vectorizer.get_feature_names())\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''words = list(simple_vectorizer.vocabulary_.keys())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concatenate DTM with original dataset using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''result1 = pd.concat([df, bow1_df], axis=1, join='inner')\n",
    "result2 = pd.concat([df_st, bow2_df], axis=1, join='inner')\n",
    "result3=  pd.concat([df_ask, bow3_df], axis=1, join='inner')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''result1.to_csv('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\df_expand1 - Copy.csv',encoding=\"utf-8\")\n",
    "df_st_expand.to_csv('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\df_st_expand2 - Copy.csv',encoding=\"utf-8\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''result2.to_csv('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\df_expand2 - Copy.csv',encoding=\"utf-8\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''result3.to_csv('C:\\\\Users\\\\zaina\\\\Documents\\\\DataVisulaization_MachineLearning\\\\Open_Stack\\\\df_expand3 - Copy.csv',encoding=\"utf-8\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Topic Modeling: This script loads a gensim dictionary and associated corpus and applies an LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim\n",
    "from optparse import OptionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build Dictinoary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opts= \"open_stack\"\n",
    "dictionary = corpora.Dictionary([mydict])\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Dictionary: \")\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save dictionary for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save(\"open_stack.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus =[dictionary.doc2bow(df.field_words_bag.loc[i]) for i in range(0,len(df))] \n",
    "corpus= corpus + [dictionary.doc2bow(df_st.field_words_bag.loc[i]) for i in range(0,len(df_st))] \n",
    "corpus= corpus +[dictionary.doc2bow(df_ask.field_words_bag.loc[i]) for i in range(0,len(df_ask))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save corpus for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpora.MmCorpus.serialize(\"open_stack.mm\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the corpus and Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus(\"open_stack.mm\")\n",
    "dictionary = corpora.Dictionary.load(\"open_stack.dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_topics,passes,alpha=5,20,0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus, id2word=dictionary,\n",
    "                        num_topics=num_topics,\n",
    "                        passes=passes,\n",
    "                        alpha =alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save lda results for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.save(\"open_stack.lda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First LDA model with 5 topics, 20 passes, alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "openstack_data =  pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(openstack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(openstack_data,\"openstack.html\")\n",
    "#pyLDAvis.save_html(vis_data,outpth+'LDA_Visualization.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_table=pd.DataFrame(openstack_data.token_table )\n",
    "token_table.to_csv(\"open_stack_token_table.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "environment": null,
   "summary": "ML_FaultGenes",
   "url": "https://anaconda.org/zainabsayyed/openstack_ml"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
